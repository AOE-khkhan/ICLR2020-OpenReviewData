{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "from util import crawl_meta, PaperMeta, Keyword\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_DATA = False\n",
    "AFTER_DECISION = False\n",
    "CRAWL_REVIEW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the meta data\n",
    "if CRAWL_DATA:\n",
    "    # Uncomment this if you want to crawl data from scratch\n",
    "    meta_list = crawl_meta(\n",
    "        meta_hdf5=None, \n",
    "        write_meta_name='data_{}.hdf5'.format(time.strftime(\"%Y%m%d%H%M%S\")), \n",
    "        crawl_review=CRAWL_REVIEW)\n",
    "else:\n",
    "    # Uncomment this if you want to load the previously stored data file\n",
    "    meta_hdf5s = glob.glob(\"*.hdf5\")\n",
    "    meta_hdf5 = sorted(meta_hdf5s)[-1] # the most recent data\n",
    "    print(\"Loading {} ...\".format(meta_hdf5))\n",
    "    meta_list = crawl_meta(meta_hdf5=meta_hdf5)\n",
    "num_withdrawn = len([m for m in meta_list if m.withdrawn or m.desk_reject])\n",
    "print('Number of submissions: {} (withdrawn/desk reject submissions: {})'.format(\n",
    "    len(meta_list), num_withdrawn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "rating_mean = []\n",
    "num_rating = []\n",
    "keywords = []\n",
    "for m in meta_list:\n",
    "    rating.extend(m.rating)\n",
    "    keywords.extend(m.keyword)\n",
    "    if not (m.withdrawn or m.desk_reject):\n",
    "        num_rating.append(len(m.rating))\n",
    "    if len(m.rating) > 0:\n",
    "        rating_mean.append(m.average_rating)\n",
    "print('Average rating: {:.4f}'.format(np.mean(rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histograms of ratings\n",
    "from collections import Counter\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns; sns.set()\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.suptitle('Histograms of ICLR 2020 Submission Ratings')\n",
    "\n",
    "# Rating hist\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 3]) \n",
    "plt.subplot(gs[0])\n",
    "counterlist =  sorted(Counter(rating).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax1.set_xlabel('Rating hist')\n",
    "ax1.set_ylabel('Number of papers')\n",
    "ax1.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax1.patches\n",
    "labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width() / 2, height + 5, label,\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Rating per paper hist\n",
    "plt.subplot(gs[1])\n",
    "counterlist =  sorted(Counter(rating_mean).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax2 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax2.set_xlabel('Rating per paper hist')\n",
    "ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "rects = ax2.patches\n",
    "labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax2.text(rect.get_x() + rect.get_width() / 2, height + 3, label,\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "plt.show()\n",
    "fig.savefig('asset/rating.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of ratings\n",
    "from collections import Counter\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns; sns.set()\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "plt.suptitle('Histograms of ICLR 2020 Submission Ratings')\n",
    "\n",
    "# Rating hist\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 7]) \n",
    "plt.subplot(gs[0])\n",
    "counterlist =  sorted(Counter(rating).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "frequencies = [float(f) / np.sum(frequencies) * 100 for f in frequencies]\n",
    "frequencies = np.cumsum(frequencies[::-1])[::-1]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax1.set_xlabel('Rating')\n",
    "ax1.set_ylabel('Percentage (%)')\n",
    "ax1.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax1.patches\n",
    "labels = ['{:.1f}'.format(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width() / 2, height + 1.5, label,\n",
    "            ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Rating per paper hist\n",
    "plt.subplot(gs[1])\n",
    "counterlist =  sorted(Counter(rating_mean).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "frequencies = [float(f) / np.sum(frequencies) * 100 for f in frequencies]\n",
    "frequencies = np.cumsum(frequencies[::-1])[::-1]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax2 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax2.set_xlabel('Average Rating')\n",
    "ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "rects = ax2.patches\n",
    "labels = ['{:.1f}'.format(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax2.text(rect.get_x() + rect.get_width() / 2, height + 1.5, label,\n",
    "            ha='center', va='bottom', fontsize=5)\n",
    "plt.show()\n",
    "fig.savefig('asset/rating_cumsum.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many papers are beaten by yours\n",
    "def PR(rating_mean, your_rating):\n",
    "    pr = np.sum(your_rating > np.array(rating_mean))/len(rating_mean)*100\n",
    "    same_rating = np.sum(your_rating == np.array(rating_mean))/len(rating_mean)*100    \n",
    "    return pr, same_rating\n",
    "my_rating = (6+6+3)/3.  # your average rating here\n",
    "pr, same_rating = PR(rating_mean, my_rating)\n",
    "print('Your papar ({:.2f}) is among the top {:.2f}% of submissions based on the ratings.\\n'\n",
    "      'There are {:.2f}% with the same rating.'.format(\n",
    "          my_rating, 100-pr, same_rating))\n",
    "\n",
    "#            accept rate       orals     posters\n",
    "# ICLR 2017: 39.1% (198/507)    15         183\n",
    "# ICLR 2018: 32.0% (314/981)    23         291\n",
    "# ICLR 2019: 31.4% (500/1591)   24         476\n",
    "# ICLR 2020: ?     (?/2594)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count keywords\n",
    "keywords_hist = Counter(keywords)\n",
    "del keywords_hist['']\n",
    "print('{} different keywords before merging'.format(len(keywords_hist)))\n",
    "\n",
    "# Merge duplicates: CNNs and CNN\n",
    "duplicates = []\n",
    "for k in keywords_hist:\n",
    "    if k+'s' in keywords_hist:\n",
    "        duplicates.append(k)\n",
    "for k in duplicates:\n",
    "    keywords_hist[k] += keywords_hist[k+'s']\n",
    "    del keywords_hist[k+'s']\n",
    "print('{} different keywords after merging'.format(len(keywords_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keywords list\n",
    "keywords_list = []\n",
    "keywords_key_list = []\n",
    "for m in meta_list:\n",
    "    for k in [mk for mk in m.keyword if not mk == '']:\n",
    "        if k not in keywords_hist.keys():\n",
    "            k = k[:-1]  # strip 's'\n",
    "        if k in keywords_key_list:\n",
    "            idx = keywords_key_list.index(k)\n",
    "            keywords_list[idx].update_frequency(1)\n",
    "            keywords_list[idx].update_rating(m.rating)\n",
    "        else:\n",
    "            # the keyword is new to the list\n",
    "            k_object = Keyword(k, 1, m.rating)\n",
    "            keywords_list.append(k_object)\n",
    "            keywords_key_list.append(k_object.keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords_list_subset = [k for k in keywords_list if k.frequency > 9]\n",
    "print(len(keywords_list_subset))\n",
    "y = [k.average_rating() for k in keywords_list_subset]\n",
    "x = [np.log2(k.frequency) for k in keywords_list_subset]\n",
    "key = [k.keyword for k in keywords_list_subset]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Frequency': x,\n",
    "    'AverageRating': y,\n",
    "    'Keyword': key\n",
    "})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "p1 = sns.regplot(data=df, x=\"Frequency\", y=\"AverageRating\", fit_reg=False, \n",
    "                 marker=\"o\", color=\"red\", logx=True, scatter_kws={'s': 8})\n",
    "for line in range(0, df.shape[0]):\n",
    "     p1.text(df.Frequency[line], df.AverageRating[line], df.Keyword[line], \n",
    "             horizontalalignment='left', \n",
    "             size='small', color='black', alpha=0.6)\n",
    "plt.show()\n",
    "fig.savefig('asset/rating_frequency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show N most common keywords and their frequencies\n",
    "num_keyowrd = 50\n",
    "keywords_hist_vis = keywords_hist.most_common(num_keyowrd)\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "\n",
    "key = [k[0] for k in keywords_hist_vis] \n",
    "value = [k[1] for k in keywords_hist_vis] \n",
    "y_pos = np.arange(len(key))\n",
    "ax.barh(y_pos, value, align='center', color='green', ecolor='black', log=True)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(key, rotation=0, fontsize=10)\n",
    "ax.invert_yaxis() \n",
    "for i, v in enumerate(value):\n",
    "    ax.text(v + 3, i + .25, str(v), color='black', fontsize=10)\n",
    "# ax.text(y_pos, value, str(value))\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('ICLR 2020 Submission Top {} Keywords'.format(num_keyowrd))\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('asset/frequency.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the word cloud forming by keywords\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(max_font_size=64, max_words=160, \n",
    "                      width=1280, height=640,\n",
    "                      background_color=\"black\").generate(' '.join(keywords))\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "fig.savefig('asset/wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the word cloud with an ICLR logo\n",
    "from imageio import imread\n",
    "logo = imread('asset/logo.png')\n",
    "wordcloud = WordCloud(max_font_size=64, max_words=300, \n",
    "                      width=1280, height=640,\n",
    "                      background_color=\"white\", mask=logo).generate(' '.join(keywords))\n",
    "fig = plt.figure(figsize=(16, 8), frameon=False)\n",
    "plt.imshow(logo)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\",  alpha=.7)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "fig.savefig('asset/logo_wordcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_valid = len(meta_list) - num_withdrawn\n",
    "num_missing_rating = num_valid*3 - np.sum(np.clip(num_rating, 0, 3))\n",
    "print('Number of missing reviews: {} ({:.4f}%)'.format(\n",
    "    num_missing_rating, 100*float(num_missing_rating)/(num_valid*3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the papers that don't have 3 reviews\n",
    "count = 0\n",
    "for m in meta_list:\n",
    "    if len(m.rating) < 3 and not (m.withdrawn or m.desk_reject):\n",
    "        count += 1\n",
    "        print('[{}] {} {} {}'.format(count, m.title, m.url, m.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "meta_list_sorted_review_len_min = sorted(\n",
    "    [m for m in meta_list if not m.review_len_min is None], \n",
    "    key=operator.attrgetter('review_len_min'))\n",
    "meta_list_sorted_review_len_max = sorted(\n",
    "    [m for m in meta_list if not m.review_len_max is None], \n",
    "    key=operator.attrgetter('review_len_max'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TOP_NUM = 10\n",
    "print('[Shortest reviews]')\n",
    "for m in meta_list_sorted_review_len_min[:TOP_NUM]:\n",
    "    print('{} words: {} {}'.format(m.review_len_min, m.url, m.title))\n",
    "print('[Longest reviews]')    \n",
    "for m in meta_list_sorted_review_len_max[:TOP_NUM]:\n",
    "    print('{} words: {} {}'.format(m.review_len_max, m.url, m.title))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average review len\n",
    "all_rating = []\n",
    "all_review_len = []\n",
    "for m in meta_list:\n",
    "    if len(m.review_len) > 0:\n",
    "        all_rating.extend(m.rating)\n",
    "        all_review_len.extend(m.review_len)        \n",
    "print('The average review length: {:.2f} words'.format(np.mean(all_review_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histograms of ratings\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.title('Histogram of ICLR 2020 Review Lengths')\n",
    "\n",
    "# Review length hist (remove the 10 shortest/longest reviews)\n",
    "interval = 20\n",
    "all_review_len_sorted_clip = sorted(all_review_len)[10:-10]\n",
    "plt.hist(all_review_len_sorted_clip, bins=[v*interval for v in list(\n",
    "    np.array(range(np.max(all_review_len_sorted_clip)//interval)))],\n",
    "    color='#990000')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('# of Reviews')\n",
    "plt.show()\n",
    "fig.savefig('asset/review_len_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ratings = np.unique(all_rating)\n",
    "bins = [v*interval for v in list(np.array(range(np.max(all_review_len)//interval)))]\n",
    "gs = gridspec.GridSpec(4, 1) \n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "for i, unique_rating in enumerate(unique_ratings):\n",
    "    plt.subplot(gs[i])\n",
    "    # Plot histograms of ratings\n",
    "    \n",
    "    # Review length hist\n",
    "    all_review_len_rating = []\n",
    "    for j, r in enumerate(all_rating):\n",
    "        if r == unique_rating:\n",
    "            all_review_len_rating.append(all_review_len[j])\n",
    "            \n",
    "    plt.title('Histogram of Review Lengths with Rating {} (the average length: {:.2f})'.format(\n",
    "        unique_rating, np.mean(all_review_len_rating)))\n",
    "    \n",
    "    plt.hist(all_review_len_rating, bins=bins, color='#990000')\n",
    "    if i == len(unique_ratings)-1:\n",
    "        plt.xlabel('Words')\n",
    "    else:\n",
    "        plt.tick_params(\n",
    "            axis='x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom=False,      # ticks along the bottom edge are off\n",
    "            top=False,         # ticks along the top edge are off\n",
    "            labelbottom=False) # labels along the bottom edge are off    \n",
    "    plt.ylim(0, 180)\n",
    "    plt.xlim(0, 1500)     \n",
    "    plt.ylabel('# of reviews')      \n",
    "plt.show()    \n",
    "fig.savefig('asset/review_len_hist_rating.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high variance (both 1 and 8 but not 3 or 6)\n",
    "count = 0\n",
    "for m in meta_list:\n",
    "    if 1 in m.rating and 8 in m.rating and 6 not in m.rating and 3 not in m.rating:\n",
    "        count += 1\n",
    "        print('[{}] {} {} {}'.format(count, m.title, m.url, m.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all the data to README\n",
    "import datetime\n",
    "with open('README.md', 'r+') as readme:\n",
    "    lines = readme.readlines()\n",
    "\n",
    "data_title = '## <a id=\"Data\"></a>All ICLR 2020 OpenReview data\\n'\n",
    "idx = lines.index(data_title)\n",
    "lines = lines[:idx]\n",
    "\n",
    "with open('README.md', 'w') as readme:\n",
    "    for line in lines:\n",
    "        readme.write(line)\n",
    "    readme.write(data_title)\n",
    "    readme.write('Collected at {}\\n\\n'.format(datetime.datetime.now()))\n",
    "    readme.write('Number of submissions: {} (withdrawn/desk reject submissions: {})\\n\\n'.format(\n",
    "        len(meta_list), num_withdrawn))\n",
    "    readme.write('| Rank | Average Rating | Title | Ratings | Variance | Decision |\\n')\n",
    "    readme.write('| --- | --- | --- | --- | --- | --- |\\n')\n",
    "    non_empty_rating_meta_list = [m for m in meta_list if not len(m.rating)==0]\n",
    "    empty_rating_meta_list = [m for m in meta_list if len(m.rating)==0]    \n",
    "    sorted_idx = np.argsort([np.mean(m.rating) for m in non_empty_rating_meta_list])[::-1]\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        m = non_empty_rating_meta_list[idx]\n",
    "        readme.write('| {} | {:.2f} | [{}]({}) | {} | {:.2f} | {} |\\n'.format(\n",
    "            i+1, np.mean(m.rating), \n",
    "            m.title if not (m.withdrawn or m.desk_reject) else '~~'+m.title+'~~',  \n",
    "            m.url, ', '.join([str(r) for r in list(m.rating)]),\n",
    "            np.std(m.rating), m.decision\n",
    "        ))\n",
    "    for i, m in enumerate(empty_rating_meta_list):\n",
    "        readme.write('| {} | {:.2f} | [{}]({}) | {} | {:.2f} | {} |\\n'.format(\n",
    "            i+1+len(non_empty_rating_meta_list), np.mean(m.rating), \n",
    "            m.title if not (m.withdrawn or m.desk_reject) else '~~'+m.title+'~~',\n",
    "            m.url, ', '.join([str(r) for r in list(m.rating)]),\n",
    "            np.std(m.rating), m.decision\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AFTER_DECISION:\n",
    "    rating_mean_accept = []\n",
    "    rating_mean_reject = []\n",
    "    for m in meta_list:\n",
    "        if len(m.rating) > 0:\n",
    "            if 'Accept' in m.decision:\n",
    "                rating_mean_accept.append(m.average_rating)\n",
    "            else:\n",
    "                rating_mean_reject.append(m.average_rating)\n",
    "    print('Average rating of accepted papers: {}'.format(np.mean(rating_mean_accept)))\n",
    "    print('Average rating of rejected papers: {}'.format(np.mean(rating_mean_reject)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AFTER_DECISION:\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    plt.title('Histograms of ICLR 2020 Submission Ratings', fontsize=15)\n",
    "\n",
    "    # accepted\n",
    "    counterlist_all = sorted(Counter(rating_mean).most_common())\n",
    "    counterlist_accept =  sorted(Counter(rating_mean_accept).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist_accept]:\n",
    "            counterlist_accept.append((c[0], 0))\n",
    "    counterlist_accept = sorted(counterlist_accept)\n",
    "    frequencies = [k[1] for k in counterlist_accept]\n",
    "    freq_series = pd.Series.from_array(frequencies)\n",
    "    bins = [k[0] for k in counterlist_all]\n",
    "\n",
    "    x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "    ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.5, width=0.9)\n",
    "\n",
    "    rects = ax1.patches\n",
    "    labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        if label > 0:\n",
    "            ax1.text(rect.get_x() + rect.get_width() / 2, height + 2, label,\n",
    "                     ha='center', va='bottom', fontsize=9, color='#990000')\n",
    "\n",
    "    # rejected\n",
    "    counterlist_reject =  sorted(Counter(rating_mean_reject).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist_reject]:\n",
    "            counterlist_reject.append((c[0], 0))\n",
    "    counterlist_reject = sorted(counterlist_reject)\n",
    "    frequencies = [k[1] for k in counterlist_reject]\n",
    "    freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "    ax2 = freq_series.plot(kind='bar', color='#000099', alpha=0.5, width=0.9)\n",
    "    ax2.set_xlabel('Rating per paper hist', fontsize=15)\n",
    "    ax2.set_ylabel('Number of papers', fontsize=15)\n",
    "    ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "    rects = ax2.patches[int(len(rects)/2):]\n",
    "    labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        if label > 0:\n",
    "            ax2.text(rect.get_x() + rect.get_width() / 2, height + 2, label,\n",
    "                     ha='center', va='bottom', fontsize=9, color='#000099')\n",
    "\n",
    "    ax1.legend(('Accepted Papers', 'Rejected Papers'), fontsize=15)\n",
    "    plt.show()\n",
    "    fig.savefig('asset/decision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data crawled at different time\n",
    "from collections import Counter\n",
    "data1 = 'data_20191105041300.hdf5'\n",
    "data2 = 'data_20191115155104.hdf5'\n",
    "\n",
    "def get_rating(filename):\n",
    "    print('Loading {} ...'.format(filename))\n",
    "    meta_list = crawl_meta(meta_hdf5=filename)    \n",
    "    rating = []    \n",
    "    rating_mean = []\n",
    "    for m in meta_list:\n",
    "        rating.extend(m.rating)\n",
    "        if len(m.rating) > 0:\n",
    "            rating_mean.append(m.average_rating)\n",
    "    print('Average rating: {:.4f}'.format(np.mean(rating)))    \n",
    "    \n",
    "    crawl_time = time.strftime(\n",
    "        '%m/%d/%Y %I:%M:%S %p', \n",
    "        time.strptime(filename.split('_')[-1].split('.')[0], '%Y%m%d%H%M%S'))\n",
    "    return rating_mean, rating, crawl_time\n",
    "\n",
    "rating_mean_1, rating_1, time1 = get_rating(data1)\n",
    "rating_mean_2, rating_2, time2 = get_rating(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_differet_plot(\n",
    "        data1, data2, name, xlabel, ylabel, figname=None,\n",
    "        legend_height_distance=0.3, legend_height_offset=1,\n",
    "    ):\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    plt.title(name, fontsize=15)\n",
    "\n",
    "\n",
    "    # data1\n",
    "    counterlist_all = sorted(Counter(data1['data']+data2['data']).most_common())\n",
    "    counterlist2 =  sorted(Counter(data1['data']).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist2]:\n",
    "            counterlist2.append((c[0], 0))\n",
    "    counterlist2 = sorted(counterlist2)\n",
    "    frequencies = [k[1] for k in counterlist2]\n",
    "    frequencies_normalized = [float(f) / np.sum(frequencies) * 100 for f in frequencies]\n",
    "    freq_series = pd.Series.from_array(frequencies_normalized)\n",
    "    bins = [k[0] for k in counterlist_all]\n",
    "\n",
    "    x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "    ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.25, width=0.9)\n",
    "\n",
    "    # rects = ax1.patches\n",
    "    rects1 = ax1.patches # [:int(len(rects)/2)]    \n",
    "    labels1 = ['{:.1f}'.format(frequencies_normalized[i]) for i in range(len(rects1))]    \n",
    "\n",
    "    # data2\n",
    "    counterlist2 =  sorted(Counter(data2['data']).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist2]:\n",
    "            counterlist2.append((c[0], 0))\n",
    "    counterlist2 = sorted(counterlist2)\n",
    "    frequencies = [k[1] for k in counterlist2]\n",
    "    frequencies_normalized = [float(f) / np.sum(frequencies) * 100 for f in frequencies]\n",
    "    freq_series = pd.Series.from_array(frequencies_normalized)\n",
    "\n",
    "    ax2 = freq_series.plot(kind='bar', color='#000099', alpha=0.25, width=0.9)\n",
    "    ax2.set_xlabel(xlabel, fontsize=15)\n",
    "    ax2.set_ylabel(ylabel, fontsize=15)\n",
    "    ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "    rects2 = ax2.patches[int(len(rects1)/2):]\n",
    "    labels2 = ['{:.1f}'.format(frequencies_normalized[i]) for i in range(len(rects2))]\n",
    "\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    for rect1, rect2, label1, label2 in zip(rects1, rects2, labels1, labels2):\n",
    "        height = max(rect1.get_height(), rect2.get_height())\n",
    "        if rect1.get_height() > rect2.get_height():\n",
    "            legend_height_offse_sign = 1\n",
    "        else:\n",
    "            legend_height_offse_sign = -1\n",
    "        if float(label1) > 0:\n",
    "            ax1.text(rect1.get_x() + rect1.get_width() / 2, \n",
    "                     height + legend_height_offset + legend_height_offse_sign*legend_height_distance, \n",
    "                     label1+'%', ha='center', va='bottom', fontsize=9, color='#990000')\n",
    "        if float(label2) > 0:\n",
    "            ax2.text(rect2.get_x() + rect2.get_width() / 2, \n",
    "                     height + legend_height_offset - legend_height_offse_sign*legend_height_distance, \n",
    "                     label2+'%', ha='center', va='bottom', fontsize=9, color='#000099')\n",
    "                \n",
    "    ax1.legend((data1['legend'], data2['legend']), fontsize=15)\n",
    "    plt.show()\n",
    "    if figname is not None:\n",
    "        fig.savefig(figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distribution_differet_plot(\n",
    "    {'data': rating_1, 'legend': time1},\n",
    "    {'data': rating_2, 'legend': time2},\n",
    "    'ICLR 2020 Submission Rating Change During Rebuttal',\n",
    "    'Rating',\n",
    "    'Percentage (%)',\n",
    "    'asset/rating_difference.png',\n",
    "    legend_height_distance=0.5,\n",
    "    legend_height_offset=0.4,    \n",
    ")        \n",
    "\n",
    "distribution_differet_plot(\n",
    "    {'data': rating_mean_1, 'legend': time1},\n",
    "    {'data': rating_mean_2, 'legend': time2},\n",
    "    'ICLR 2020 Submission Average Rating Change During Rebuttal',\n",
    "    'Average Rating',\n",
    "    'Percentage (%)',\n",
    "    'asset/rating_mean_difference.png',\n",
    "    legend_height_distance=0.2,\n",
    "    legend_height_offset=0.2,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a linux script that can downloads all the paper pdf with its title\n",
    "with open('download_all_paper.sh', 'w') as f:\n",
    "    for m in meta_list:\n",
    "        data_id = m.url.split('=')[-1]\n",
    "        f.write('curl -o {}.pdf https://openreview.net/pdf?id={}\\n'.format(m.title.replace(' ', '_'), data_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
