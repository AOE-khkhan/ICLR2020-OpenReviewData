{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFTER_DECISION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta data of papers\n",
    "class PaperMeta(object):\n",
    "    def __init__(self, title, abstract, keyword, rating, url, withdrawn, desk_reject, decision):\n",
    "        self.title = title  # str\n",
    "        self.abstract = abstract  # str\n",
    "        self.keyword = keyword  # list[str]\n",
    "        self.rating = rating  # list[int]\n",
    "        self.url = url\n",
    "        self.withdrawn = withdrawn\n",
    "        self.desk_reject = desk_reject        \n",
    "        self.decision = decision\n",
    "        \n",
    "        if len(self.rating) > 0:\n",
    "            self.average_rating = np.mean(rating)\n",
    "        else:\n",
    "            self.average_rating = -1\n",
    "\n",
    "            \n",
    "class Keyword(object):\n",
    "    def __init__(self, keyword, frequency, rating):\n",
    "        self.keyword = keyword  # list[str]\n",
    "        self.frequency = frequency\n",
    "        self.rating = rating  # list[int]        \n",
    "    \n",
    "    def average_rating(self):\n",
    "        if len(self.rating) > 0:\n",
    "            return np.mean(self.rating)\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def update_frequency(self, frequency):\n",
    "        self.frequency += frequency\n",
    "        \n",
    "    def update_rating(self, rating):\n",
    "        self.rating = np.concatenate((self.rating, rating))\n",
    "            \n",
    "            \n",
    "def write_meta(meta_list, filename):\n",
    "    f = h5py.File(filename, 'w')\n",
    "    for i, m in enumerate(meta_list):\n",
    "        grp = f.create_group(str(i))\n",
    "        grp['title'] = m.title\n",
    "        grp['abstract'] = m.abstract\n",
    "        grp['keyword'] = '#'.join(m.keyword)\n",
    "        grp['rating'] = m.rating\n",
    "        grp['url'] = m.url\n",
    "        grp['withdrawn'] = m.withdrawn \n",
    "        grp['desk_reject'] = m.desk_reject         \n",
    "        grp['decision'] = m.decision\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def read_meta(filename):\n",
    "    f = h5py.File(filename, 'r')\n",
    "    meta_list = []\n",
    "    for k in list(f.keys()):\n",
    "        meta_list.append(PaperMeta(\n",
    "            f[k]['title'].value, \n",
    "            f[k]['abstract'].value, \n",
    "            f[k]['keyword'].value.split('#'),\n",
    "            f[k]['rating'].value,\n",
    "            f[k]['url'].value,\n",
    "            f[k]['withdrawn'].value,            \n",
    "            f[k]['desk_reject'].value,                        \n",
    "            f[k]['decision'].value,                        \n",
    "        ))\n",
    "    return meta_list\n",
    "\n",
    "\n",
    "def crawl_meta(meta_hdf5=None, write_meta_name='data.hdf5'):\n",
    "    \n",
    "    if meta_hdf5 is None:\n",
    "        # Crawl the meta data from OpenReview\n",
    "        # Set up a browser to crawl from dynamic web pages \n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        \n",
    "        # from pyvirtualdisplay import Display\n",
    "        # display = Display(visible=0, size=(800, 800))\n",
    "        # display.start()\n",
    "        \n",
    "        import time\n",
    "        executable_path = '/usr/local/bin/chromedriver'\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        browser = webdriver.Chrome(options=options, executable_path=executable_path)            \n",
    "    \n",
    "        # Load all URLs for all ICLR submissions\n",
    "        urls = []\n",
    "        with open('urls.txt') as f:\n",
    "            urls = f.readlines()\n",
    "        urls = [url.strip() for url in urls]\n",
    "        \n",
    "        meta_list = [] \n",
    "        wait_time = 1.0\n",
    "        max_try = 1000\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                browser.get(url)\n",
    "                time.sleep(wait_time)\n",
    "                key = browser.find_elements_by_class_name(\"note_content_field\")\n",
    "                key = [k.text for k in key]\n",
    "                withdrawn = 'Withdrawal Confirmation:' in key\n",
    "                desk_reject = 'Desk Reject Comments:' in key\n",
    "                value = browser.find_elements_by_class_name(\"note_content_value\")\n",
    "                value = [v.text for v in value]\n",
    "\n",
    "                # title\n",
    "                title = string.capwords(browser.find_element_by_class_name(\"note_content_title\").text)\n",
    "                # abstract\n",
    "                valid = False\n",
    "                tries = 0\n",
    "                while not valid:\n",
    "                    if 'Abstract:' in key:\n",
    "                        valid = True\n",
    "                    else:\n",
    "                        time.sleep(wait_time)\n",
    "                        tries += 1\n",
    "                        key = browser.find_elements_by_class_name(\"note_content_field\")\n",
    "                        key = [k.text for k in key]\n",
    "                        withdrawn = 'Withdrawal Confirmation:' in key\n",
    "                        value = browser.find_elements_by_class_name(\"note_content_value\")\n",
    "                        value = [v.text for v in value]                        \n",
    "                        if tries >= max_try:\n",
    "                            print('Reached max try: {} ({})'.format(title, url))\n",
    "                            break\n",
    "                abstract = ' '.join(value[key.index('Abstract:')].split('\\n'))\n",
    "                # keyword\n",
    "                if 'Keywords:' in key:\n",
    "                    keyword = value[key.index('Keywords:')].split(',')\n",
    "                    keyword = [k.strip(' ') for k in keyword]\n",
    "                    keyword = [''.join(string.capwords(k).split(' ')) for k in keyword if not k == '']\n",
    "                    for j in range(len(keyword)):\n",
    "                        if '-' in keyword[j]:\n",
    "                            keyword[j] = ''.join([string.capwords(kk) for kk in keyword[j].split('-')])       \n",
    "                else:\n",
    "                    keyword = []\n",
    "                # rating\n",
    "                rating_idx = [i for i, x in enumerate(key) if x == \"Rating:\"]\n",
    "                rating = []\n",
    "                if len(rating_idx) > 0:\n",
    "                    for idx in rating_idx:\n",
    "                        rating.append(int(value[idx].split(\":\")[0]))\n",
    "                # decision\n",
    "                if 'Recommendation:' in key:\n",
    "                    decision = value[key.index('Recommendation:')]\n",
    "                else:\n",
    "                    decision = 'N/A'\n",
    "                \n",
    "                withdrawn_or_desk_reject = withdrawn or desk_reject\n",
    "                \n",
    "                print('[{}] Abs: {} chars, keywords: {}, ratings: {}{} {}{}'.format(\n",
    "                    i+1, len(abstract), len(keyword), rating, \n",
    "                    '' if not AFTER_DECISION else ', decision: {}'.format(decision), \n",
    "                    title, '' if not withdrawn_or_desk_reject else ' ({})'.format(\n",
    "                        'withdrawn' if withdrawn else 'desk reject'\n",
    "                    ))\n",
    "                )\n",
    "                meta_list.append(PaperMeta(title, abstract, keyword, rating, url, \n",
    "                                           withdrawn, desk_reject, decision))\n",
    "            except:\n",
    "                print('Failed to load {}'.format(url))\n",
    "            \n",
    "        # Save the crawled data\n",
    "        write_meta(meta_list, write_meta_name)\n",
    "    else:\n",
    "        # Load the meta data from local\n",
    "        meta_list = read_meta(meta_hdf5)\n",
    "    return meta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/walter/ENV/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'rating' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4ee1637a1868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the meta data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Uncomment this if you want to load the previously stored data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmeta_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Uncomment this if you want to cral data from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# meta_list = crawl_meta()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-280068954646>\u001b[0m in \u001b[0;36mcrawl_meta\u001b[0;34m(meta_hdf5, write_meta_name)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Load the meta data from local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mmeta_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_hdf5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-280068954646>\u001b[0m in \u001b[0;36mread_meta\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'withdrawn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'rating' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "# Get the meta data\n",
    "# Uncomment this if you want to load the previously stored data file\n",
    "meta_list = crawl_meta('data.hdf5')\n",
    "# Uncomment this if you want to cral data from scratch\n",
    "# meta_list = crawl_meta()\n",
    "num_withdrawn = len([m for m in meta_list if m.withdrawn or m.desk_reject])\n",
    "print('Number of submissions: {} (withdrawn/desk reject submissions: {})'.format(\n",
    "    len(meta_list), num_withdrawn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "rating_mean = []\n",
    "num_rating = []\n",
    "keywords = []\n",
    "for m in meta_list:\n",
    "    rating.extend(m.rating)\n",
    "    keywords.extend(m.keyword)\n",
    "    if not (m.withdrawn or m.desk_reject):\n",
    "        num_rating.append(len(m.rating))\n",
    "    if len(m.rating) > 0:\n",
    "        rating_mean.append(m.average_rating)\n",
    "print('Average rating: {}'.format(np.mean(rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histograms of ratings\n",
    "from collections import Counter\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns; sns.set()\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.suptitle('Histograms of ICLR 2020 Submission Ratings')\n",
    "\n",
    "# Rating hist\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 3]) \n",
    "plt.subplot(gs[0])\n",
    "counterlist =  sorted(Counter(rating).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax1.set_xlabel('Rating hist')\n",
    "ax1.set_ylabel('Number of papers')\n",
    "ax1.set_xticklabels(x_labels)\n",
    "\n",
    "rects = ax1.patches\n",
    "labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax1.text(rect.get_x() + rect.get_width() / 2, height + 5, label,\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Rating per paper hist\n",
    "plt.subplot(gs[1])\n",
    "counterlist =  sorted(Counter(rating_mean).most_common())\n",
    "frequencies = [k[1] for k in counterlist]\n",
    "bins = [k[0] for k in counterlist]\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "ax2 = freq_series.plot(kind='bar', color='#990000', alpha=0.75, width=0.9)\n",
    "ax2.set_xlabel('Rating per paper hist')\n",
    "ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "rects = ax2.patches\n",
    "labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax2.text(rect.get_x() + rect.get_width() / 2, height + 3, label,\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many papers are beaten by yours\n",
    "def PR(rating_mean, your_rating):\n",
    "    pr = np.sum(your_rating > np.array(rating_mean))/len(rating_mean)*100\n",
    "    return pr\n",
    "my_rating = (6+6+3)/3.  # your average rating here\n",
    "print('Your papar ({:.2f}) beats {:.2f}% of submissions based on the ratings.'.format(\n",
    "          my_rating, PR(rating_mean, my_rating)))\n",
    "\n",
    "#            accept rate       orals     posters\n",
    "# ICLR 2017: 39.1% (198/507)    15         183\n",
    "# ICLR 2018: 32.0% (314/981)    23         291\n",
    "# ICLR 2019: 31.4% (500/1591)   24         476\n",
    "# ICLR 2020: ?     (?/2594)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count keywords\n",
    "keywords_hist = Counter(keywords)\n",
    "del keywords_hist['']\n",
    "print('{} different keywords before merging'.format(len(keywords_hist)))\n",
    "\n",
    "# Merge duplicates: CNNs and CNN\n",
    "duplicates = []\n",
    "for k in keywords_hist:\n",
    "    if k+'s' in keywords_hist:\n",
    "        duplicates.append(k)\n",
    "for k in duplicates:\n",
    "    keywords_hist[k] += keywords_hist[k+'s']\n",
    "    del keywords_hist[k+'s']\n",
    "print('{} different keywords after merging'.format(len(keywords_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keywords list\n",
    "keywords_list = []\n",
    "keywords_key_list = []\n",
    "for m in meta_list:\n",
    "    for k in [mk for mk in m.keyword if not mk == '']:\n",
    "        if k not in keywords_hist.keys():\n",
    "            k = k[:-1]  # strip 's'\n",
    "        if k in keywords_key_list:\n",
    "            idx = keywords_key_list.index(k)\n",
    "            keywords_list[idx].update_frequency(1)\n",
    "            keywords_list[idx].update_rating(m.rating)\n",
    "        else:\n",
    "            # the keyword is new to the list\n",
    "            k_object = Keyword(k, 1, m.rating)\n",
    "            keywords_list.append(k_object)\n",
    "            keywords_key_list.append(k_object.keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords_list_subset = [k for k in keywords_list if k.frequency > 9]\n",
    "print(len(keywords_list_subset))\n",
    "y = [k.average_rating() for k in keywords_list_subset]\n",
    "x = [np.log2(k.frequency) for k in keywords_list_subset]\n",
    "key = [k.keyword for k in keywords_list_subset]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Frequency': x,\n",
    "    'AverageRating': y,\n",
    "    'Keyword': key\n",
    "})\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "p1 = sns.regplot(data=df, x=\"Frequency\", y=\"AverageRating\", fit_reg=False, \n",
    "                 marker=\"o\", color=\"red\", logx=True, scatter_kws={'s': 8})\n",
    "for line in range(0, df.shape[0]):\n",
    "     p1.text(df.Frequency[line], df.AverageRating[line], df.Keyword[line], \n",
    "             horizontalalignment='left', \n",
    "             size='small', color='black', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show N most common keywords and their frequencies\n",
    "num_keyowrd = 50\n",
    "keywords_hist_vis = keywords_hist.most_common(num_keyowrd)\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(8, 12))\n",
    "\n",
    "key = [k[0] for k in keywords_hist_vis] \n",
    "value = [k[1] for k in keywords_hist_vis] \n",
    "y_pos = np.arange(len(key))\n",
    "ax.barh(y_pos, value, align='center', color='green', ecolor='black', log=True)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(key, rotation=0, fontsize=10)\n",
    "ax.invert_yaxis() \n",
    "for i, v in enumerate(value):\n",
    "    ax.text(v + 3, i + .25, str(v), color='black', fontsize=10)\n",
    "# ax.text(y_pos, value, str(value))\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('ICLR 2020 Submission Top {} Keywords'.format(num_keyowrd))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the word cloud forming by keywords\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(max_font_size=64, max_words=160, \n",
    "                      width=1280, height=640,\n",
    "                      background_color=\"black\").generate(' '.join(keywords))\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the word cloud with an ICLR logo\n",
    "from imageio import imread\n",
    "logo = imread('asset/logo.png')\n",
    "wordcloud = WordCloud(max_font_size=64, max_words=300, \n",
    "                      width=1280, height=640,\n",
    "                      background_color=\"white\", mask=logo).generate(' '.join(keywords))\n",
    "plt.figure(figsize=(16, 8), frameon=False)\n",
    "plt.imshow(logo)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\",  alpha=.7)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_valid = len(meta_list) - num_withdrawn\n",
    "num_missing_rating = num_valid*3 - np.sum(np.clip(num_rating, 0, 3))\n",
    "print('Number of missing reviews: {} ({:.4f}%)'.format(\n",
    "    num_missing_rating, 100*float(num_missing_rating)/(num_valid*3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the papers that don't have 3 reviews\n",
    "for m in meta_list:\n",
    "    if len(m.rating) < 3 and not (m.withdrawn or m.desk_reject):\n",
    "        print(m.title, m.url, m.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high variance (both 1 and 8 but not 3 or 6)\n",
    "for m in meta_list:\n",
    "    if 1 in m.rating and 8 in m.rating and 6 not in m.rating and 3 not in m.rating:\n",
    "        print('{} ({}) {}'.format(m.title, m.url, m.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all the data to README\n",
    "import datetime\n",
    "with open('README.md', 'r+') as readme:\n",
    "    lines = readme.readlines()\n",
    "\n",
    "data_title = '## <a id=\"Data\"></a>All ICLR 2020 OpenReview data\\n'\n",
    "idx = lines.index(data_title)\n",
    "lines = lines[:idx]\n",
    "\n",
    "with open('README.md', 'w') as readme:\n",
    "    for line in lines:\n",
    "        readme.write(line)\n",
    "    readme.write(data_title)\n",
    "    readme.write('Collected at {}\\n\\n'.format(datetime.datetime.now()))\n",
    "    readme.write('Number of submissions: {} (withdrawn/desk reject submissions: {})\\n\\n'.format(\n",
    "        len(meta_list), num_withdrawn))\n",
    "    readme.write('| Rank | Average Rating | Title | Ratings | Variance | Decision |\\n')\n",
    "    readme.write('| --- | --- | --- | --- | --- | --- |\\n')\n",
    "    non_empty_rating_meta_list = [m for m in meta_list if not len(m.rating)==0]\n",
    "    empty_rating_meta_list = [m for m in meta_list if len(m.rating)==0]    \n",
    "    sorted_idx = np.argsort([np.mean(m.rating) for m in non_empty_rating_meta_list])[::-1]\n",
    "    for i, idx in enumerate(sorted_idx):\n",
    "        m = non_empty_rating_meta_list[idx]\n",
    "        readme.write('| {} | {:.2f} | [{}]({}) | {} | {:.2f} | {} |\\n'.format(\n",
    "            i+1, np.mean(m.rating), \n",
    "            m.title if not (m.withdrawn or m.desk_reject) else '~~'+m.title+'~~',  \n",
    "            m.url, ', '.join([str(r) for r in list(m.rating)]),\n",
    "            np.std(m.rating), m.decision\n",
    "        ))\n",
    "    for i, m in enumerate(empty_rating_meta_list):\n",
    "        readme.write('| {} | {:.2f} | [{}]({}) | {} | {:.2f} | {} |\\n'.format(\n",
    "            i+1+len(non_empty_rating_meta_list), np.mean(m.rating), \n",
    "            m.title if not (m.withdrawn or m.desk_reject) else '~~'+m.title+'~~',\n",
    "            m.url, ', '.join([str(r) for r in list(m.rating)]),\n",
    "            np.std(m.rating), m.decision\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AFTER_DECISION:\n",
    "    rating_mean_accept = []\n",
    "    rating_mean_reject = []\n",
    "    for m in meta_list:\n",
    "        if len(m.rating) > 0:\n",
    "            if 'Accept' in m.decision:\n",
    "                rating_mean_accept.append(m.average_rating)\n",
    "            else:\n",
    "                rating_mean_reject.append(m.average_rating)\n",
    "    print('Average rating of accepted papers: {}'.format(np.mean(rating_mean_accept)))\n",
    "    print('Average rating of rejected papers: {}'.format(np.mean(rating_mean_reject)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AFTER_DECISION:\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    plt.title('Histograms of ICLR 2020 Submission Ratings', fontsize=15)\n",
    "\n",
    "    # accepted\n",
    "    counterlist_all = sorted(Counter(rating_mean).most_common())\n",
    "    counterlist_accept =  sorted(Counter(rating_mean_accept).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist_accept]:\n",
    "            counterlist_accept.append((c[0], 0))\n",
    "    counterlist_accept = sorted(counterlist_accept)\n",
    "    frequencies = [k[1] for k in counterlist_accept]\n",
    "    freq_series = pd.Series.from_array(frequencies)\n",
    "    bins = [k[0] for k in counterlist_all]\n",
    "\n",
    "    x_labels = ['{:.2f}'.format(b) for b in bins]\n",
    "\n",
    "    ax1 = freq_series.plot(kind='bar', color='#990000', alpha=0.5, width=0.9)\n",
    "\n",
    "    rects = ax1.patches\n",
    "    labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        if label > 0:\n",
    "            ax1.text(rect.get_x() + rect.get_width() / 2, height + 2, label,\n",
    "                     ha='center', va='bottom', fontsize=9, color='#990000')\n",
    "\n",
    "    # rejected\n",
    "    counterlist_reject =  sorted(Counter(rating_mean_reject).most_common())\n",
    "    for c in counterlist_all:\n",
    "        if c[0] not in [ca[0] for ca in counterlist_reject]:\n",
    "            counterlist_reject.append((c[0], 0))\n",
    "    counterlist_reject = sorted(counterlist_reject)\n",
    "    frequencies = [k[1] for k in counterlist_reject]\n",
    "    freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "    ax2 = freq_series.plot(kind='bar', color='#000099', alpha=0.5, width=0.9)\n",
    "    ax2.set_xlabel('Rating per paper hist', fontsize=15)\n",
    "    ax2.set_ylabel('Number of papers', fontsize=15)\n",
    "    ax2.set_xticklabels(x_labels, fontsize=9)\n",
    "\n",
    "    rects = ax2.patches[int(len(rects)/2):]\n",
    "    labels = [int(frequencies[i]) for i in range(len(rects))]\n",
    "\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        if label > 0:\n",
    "            ax2.text(rect.get_x() + rect.get_width() / 2, height + 2, label,\n",
    "                     ha='center', va='bottom', fontsize=9, color='#000099')\n",
    "\n",
    "    ax1.legend(('Accepted Papers', 'Rejected Papers'), fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
